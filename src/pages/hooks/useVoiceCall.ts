import { useState, useRef, useEffect, useCallback } from 'react'
import { saluteFrontendService } from '../../service/api/saluteFrontendService'
import { interviewService } from '../../service/api/interviewService'
import { socketService } from '../../service/realtime/socketService'

const AVAILABLE_VOICES = [
  'Nec_24000', 'Bys_24000',
  'Tur_24000', 'Ost_24000',
  'Pon_24000', 'Bin_24000',
  'May_24000'
]

interface UseVoiceCallReturn {
  isRecording: boolean
  isAIThinking: boolean
  isAISpeaking: boolean
  isMicrophoneBlocked: boolean
  toggleRecording: () => void
  transcript: string
  aiResponse: string
  error: string | null
}

export const useVoiceCall = (
  sessionId: string,
  position: string,
  isCodeTaskActive: boolean,
  isInterviewEnded: boolean
): UseVoiceCallReturn => {
  const [isRecording, setIsRecording] = useState(false)
  const [isAIThinking, setIsAIThinking] = useState(false)
  const [isAISpeaking, setIsAISpeaking] = useState(false)
  const [transcript, setTranscript] = useState('')
  const [aiResponse, setAiResponse] = useState('')
  const [error, setError] = useState<string | null>(null)

  // –ë–ª–æ–∫–∏—Ä—É–µ–º –º–∏–∫—Ä–æ—Ñ–æ–Ω, –µ—Å–ª–∏ –ò–ò –¥—É–º–∞–µ—Ç –∏–ª–∏ –≥–æ–≤–æ—Ä–∏—Ç
  const isMicrophoneBlocked = isAIThinking || isAISpeaking || isInterviewEnded

  const audioContextRef = useRef<AudioContext | null>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)
  const scriptProcessorRef = useRef<ScriptProcessorNode | null>(null)
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null)
  const audioChunksRef = useRef<Float32Array[]>([])
  const isCodeTaskActiveRef = useRef(isCodeTaskActive)

  // –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≥–æ–ª–æ—Å –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ö—É–∫–∞
  useEffect(() => {
    const randomVoice = AVAILABLE_VOICES[Math.floor(Math.random() * AVAILABLE_VOICES.length)]
    // –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –µ–≥–æ –≤ —Å–µ—Ä–≤–∏—Å
    saluteFrontendService.setVoice(randomVoice)

    console.log(`üé§ –í—ã–±—Ä–∞–Ω –≥–æ–ª–æ—Å —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞: ${randomVoice}`)
  }, [])

  // –ï—Å–ª–∏ –Ω–∞—á–∞–ª–∞—Å—å –∑–∞–¥–∞—á–∞ –∏–ª–∏ –∏–Ω—Ç–µ—Ä–≤—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å
  useEffect(() => {
    isCodeTaskActiveRef.current = isCodeTaskActive
    if ((isCodeTaskActive || isInterviewEnded) && isRecording) {
      stopRecording()
    }
  }, [isCodeTaskActive, isInterviewEnded])

  // –≠—Ñ—Ñ–µ–∫—Ç –¥–ª—è –ø–æ–¥–ø–∏—Å–∫–∏ –Ω–∞ —Å–æ–±—ã—Ç–∏—è –ê—É–¥–∏–æ-—Å–µ—Ä–≤–∏—Å–∞
  useEffect(() => {
    saluteFrontendService.setAudioEndListener(() => {
      setIsAISpeaking(false)
    })

    return () => {
      saluteFrontendService.setAudioEndListener(() => {})
    }
  }, [])

  const startRecording = useCallback(async () => {
    // –°—Ç—Ä–æ–≥–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞
    if (isMicrophoneBlocked) {
      console.warn('–ú–∏–∫—Ä–æ—Ñ–æ–Ω –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω')
      return
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      mediaStreamRef.current = stream
      const audioContext = new AudioContext()
      await audioContext.resume()
      audioContextRef.current = audioContext

      const source = audioContext.createMediaStreamSource(stream)
      sourceRef.current = source
      const processor = audioContext.createScriptProcessor(4096, 1, 1)
      scriptProcessorRef.current = processor
      audioChunksRef.current = []

      processor.onaudioprocess = (e) => {
        const inputData = e.inputBuffer.getChannelData(0)
        audioChunksRef.current.push(new Float32Array(inputData))
      }

      source.connect(processor)
      processor.connect(audioContext.destination)
      setIsRecording(true)
      setError(null)
    } catch (err) {
      console.error(err)
      setError('–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É')
    }
  }, [isMicrophoneBlocked])

  const stopRecording = useCallback(async () => {
    if (!isRecording) return
    setIsRecording(false)

    // –ï—Å–ª–∏ –∏–Ω—Ç–µ—Ä–≤—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –Ω–µ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º "–¥—É–º–∞–µ—Ç"
    if (!isInterviewEnded) {
      setIsAIThinking(true)
    }

    const buffers = audioChunksRef.current
    const totalLength = buffers.reduce((acc, b) => acc + b.length, 0)
    const result = new Float32Array(totalLength)
    let offset = 0
    for(const b of buffers) { result.set(b, offset); offset += b.length }

    const resampled = downsampleBuffer(result, audioContextRef.current?.sampleRate || 44100, 16000)
    const pcm = convertFloatTo16BitPCM(resampled)

    stopAudioCapture()

    const blob = new Blob([pcm], { type: 'application/octet-stream' })

    try {
      const text = await saluteFrontendService.recognizeAudio(blob)
      if(text) {
        setTranscript(text)
        socketService.sendTranscript(sessionId, text, position)
        // isAIThinking –æ—Å—Ç–∞–µ—Ç—Å—è true -> –º–∏–∫—Ä–æ—Ñ–æ–Ω –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –¥–æ –æ—Ç–≤–µ—Ç–∞
      } else {
        setIsAIThinking(false)
      }
    } catch(e) {
      console.error(e)
      setIsAIThinking(false)
      setError('–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏')
    }
  }, [isRecording, sessionId, position, isInterviewEnded])

  // –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–µ—á–∏ –ò–ò
  useEffect(() => {
    // –ï—Å–ª–∏ –ò–ò –∑–∞–∫–æ–Ω—á–∏–ª –≥–æ–≤–æ—Ä–∏—Ç—å –∏ –¥—É–º–∞—Ç—å, –∏ –º–∏–∫—Ä–æ—Ñ–æ–Ω –Ω–µ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç, –≤–∫–ª—é—á–∞–µ–º –µ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    if (
      !isAISpeaking &&
      !isAIThinking &&
      !isRecording &&
      !isInterviewEnded &&
      sessionId &&
      position &&
      socketService.getConnectionState() === 'connected'
    ) {
      const autoStartTimeout = setTimeout(() => {
        if (
          !isAISpeaking &&
          !isAIThinking &&
          !isRecording &&
          !isInterviewEnded &&
          socketService.getConnectionState() === 'connected'
        ) {
          startRecording()
        }
      }, 500)

      return () => clearTimeout(autoStartTimeout)
    }
  }, [isAISpeaking, isAIThinking, isRecording, sessionId, position, startRecording, isInterviewEnded])

  useEffect(() => {
    if (!sessionId || !position) return

    const initSocket = async () => {
      try {
        if (socketService.getConnectionState() === 'disconnected') {
          await interviewService.startInterview(sessionId, position)
        }

        // –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω—ã—Ö –ø–æ–ª–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        socketService.onMessage(async (data) => {
          setIsAIThinking(false)
          setAiResponse(data.text)
          setIsAISpeaking(true)
          await saluteFrontendService.playAudioFromText(data.text)
        })

        // --- STREAMING HANDLERS ---

        socketService.onStreamStart(() => {
          setIsAIThinking(false)
          setIsAISpeaking(true)
          setAiResponse("")
        })

        socketService.onStreamChunk((textChunk) => {
          setAiResponse(prev => prev + textChunk)
          saluteFrontendService.playAudioFromText(textChunk)
        })

        socketService.onStreamEnd(() => {
          // Stream ended
        })
      } catch (error) {
        setError(`–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —Å–µ—Ä–≤–µ—Ä—É: ${error}`)
      }
    }

    initSocket()

    return () => {
      interviewService.cleanup()
      saluteFrontendService.stopAudio()
      stopAudioCapture()
    }
  }, [sessionId, position])

  const stopAudioCapture = () => {
    if (scriptProcessorRef.current) {
      scriptProcessorRef.current.disconnect(); scriptProcessorRef.current = null
    }
    if (sourceRef.current) {
      sourceRef.current.disconnect(); sourceRef.current = null
    }
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop()); mediaStreamRef.current = null
    }
    if (audioContextRef.current) {
      audioContextRef.current.close(); audioContextRef.current = null
    }
  }

  const toggleRecording = useCallback(() => {
    if (!isRecording && isMicrophoneBlocked) return

    if (isRecording) {
      stopRecording()
    } else {
      startRecording()
    }
  }, [isRecording, startRecording, stopRecording, isMicrophoneBlocked])

  return {
    isRecording,
    isAIThinking,
    isAISpeaking,
    isMicrophoneBlocked,
    toggleRecording,
    transcript,
    aiResponse,
    error
  }
}

// Helpers
function downsampleBuffer(buffer: Float32Array, sampleRate: number, outSampleRate: number): Float32Array {
  if (outSampleRate === sampleRate) return buffer
  if (outSampleRate > sampleRate) return buffer
  const sampleRateRatio = sampleRate / outSampleRate
  const newLength = Math.round(buffer.length / sampleRateRatio)
  const result = new Float32Array(newLength)
  let offsetResult = 0
  let offsetBuffer = 0
  while (offsetResult < result.length) {
    const nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio)
    let accum = 0, count = 0
    for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
      accum += buffer[i]; count++
    }
    result[offsetResult] = count > 0 ? accum / count : 0
    offsetResult++; offsetBuffer = nextOffsetBuffer
  }
  return result
}

function convertFloatTo16BitPCM(buffer: Float32Array): ArrayBuffer {
  const l = buffer.length
  const buffer16 = new ArrayBuffer(l * 2)
  const view = new DataView(buffer16)
  for (let i = 0; i < l; i++) {
    let s = Math.max(-1, Math.min(1, buffer[i]))
    s = s < 0 ? s * 0x8000 : s * 0x7FFF
    view.setInt16(i * 2, s, true)
  }
  return buffer16
}
