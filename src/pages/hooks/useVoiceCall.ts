import { useState, useRef, useEffect, useCallback } from 'react'
import { saluteFrontendService } from '../../service/api/saluteFrontendService'
import { interviewService } from '../../service/api/interviewService'
import { socketService } from '../../service/realtime/socketService'

interface UseVoiceCallReturn {
  isRecording: boolean
  isAIThinking: boolean
  isAISpeaking: boolean
  isMicrophoneBlocked: boolean
  toggleRecording: () => void
  transcript: string
  aiResponse: string
  error: string | null
}

export const useVoiceCall = (
  sessionId: string,
  position: string,
  isCodeTaskActive: boolean
): UseVoiceCallReturn => {
  const [isRecording, setIsRecording] = useState(false)
  const [isAIThinking, setIsAIThinking] = useState(false)
  const [isAISpeaking, setIsAISpeaking] = useState(false)
  const [transcript, setTranscript] = useState('')
  const [aiResponse, setAiResponse] = useState('')
  const [error, setError] = useState<string | null>(null)

  // –ë–ª–æ–∫–∏—Ä—É–µ–º –º–∏–∫—Ä–æ—Ñ–æ–Ω, –µ—Å–ª–∏ –ò–ò –¥—É–º–∞–µ—Ç –∏–ª–∏ –≥–æ–≤–æ—Ä–∏—Ç
  const isMicrophoneBlocked = isAIThinking || isAISpeaking

  const audioContextRef = useRef<AudioContext | null>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)
  const scriptProcessorRef = useRef<ScriptProcessorNode | null>(null)
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null)
  const audioChunksRef = useRef<Float32Array[]>([])
  const isCodeTaskActiveRef = useRef(isCodeTaskActive)

  useEffect(() => {
    isCodeTaskActiveRef.current = isCodeTaskActive
    // –ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ –Ω–∞—á–∞–ª–∞—Å—å, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å
    if (isCodeTaskActive && isRecording) {
      stopRecording()
    }
  }, [isCodeTaskActive])

  // –≠—Ñ—Ñ–µ–∫—Ç –¥–ª—è –ø–æ–¥–ø–∏—Å–∫–∏ –Ω–∞ —Å–æ–±—ã—Ç–∏—è –ê—É–¥–∏–æ-—Å–µ—Ä–≤–∏—Å–∞
  useEffect(() => {
    // –ö–æ–≥–¥–∞ –∞—É–¥–∏–æ-—Å–µ—Ä–≤–∏—Å –≥–æ–≤–æ—Ä–∏—Ç "—è –≤—Å—ë", –º—ã —Å–Ω–∏–º–∞–µ–º —Ñ–ª–∞–≥ –≥–æ–≤–æ—Ä–µ–Ω–∏—è
    saluteFrontendService.setAudioEndListener(() => {
      console.log('üîä Audio playback ended, unlocking microphone')
      setIsAISpeaking(false)
    })

    return () => {
      // –û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ —Ä–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
      saluteFrontendService.setAudioEndListener(() => {})
    }
  }, [])
  const startRecording = useCallback(async () => {
    // –°—Ç—Ä–æ–≥–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞
    if (isMicrophoneBlocked) {
      console.warn('–ú–∏–∫—Ä–æ—Ñ–æ–Ω –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω, –ø–æ–∫–∞ –ò–ò –∞–∫—Ç–∏–≤–µ–Ω')
      return
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      mediaStreamRef.current = stream
      const audioContext = new AudioContext()
      await audioContext.resume()
      audioContextRef.current = audioContext

      const source = audioContext.createMediaStreamSource(stream)
      sourceRef.current = source
      const processor = audioContext.createScriptProcessor(4096, 1, 1)
      scriptProcessorRef.current = processor
      audioChunksRef.current = []

      processor.onaudioprocess = (e) => {
        const inputData = e.inputBuffer.getChannelData(0)
        audioChunksRef.current.push(new Float32Array(inputData))
      }

      source.connect(processor)
      processor.connect(audioContext.destination)
      setIsRecording(true)
      setError(null)
    } catch (err) {
      console.error(err)
      setError('–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É')
    }
  }, [isMicrophoneBlocked])

  const stopRecording = useCallback(async () => {
    if (!isRecording) return
    setIsRecording(false)
    setIsAIThinking(true) // –ë–ª–æ–∫–∏—Ä—É–µ–º –º–∏–∫—Ä–æ—Ñ–æ–Ω, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º –æ–∂–∏–¥–∞–Ω–∏—è

    const buffers = audioChunksRef.current
    const totalLength = buffers.reduce((acc, b) => acc + b.length, 0)
    const result = new Float32Array(totalLength)
    let offset = 0
    for(const b of buffers) { result.set(b, offset); offset += b.length }

    const resampled = downsampleBuffer(result, audioContextRef.current?.sampleRate || 44100, 16000)
    const pcm = convertFloatTo16BitPCM(resampled)

    stopAudioCapture()

    const blob = new Blob([pcm], { type: 'application/octet-stream' })

    try {
      const text = await saluteFrontendService.recognizeAudio(blob)
      if(text) {
        setTranscript(text)
        socketService.sendTranscript(sessionId, text, position)
        // isAIThinking –æ—Å—Ç–∞–µ—Ç—Å—è true -> –º–∏–∫—Ä–æ—Ñ–æ–Ω –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –¥–æ –æ—Ç–≤–µ—Ç–∞
      } else {
        setIsAIThinking(false) // –ï—Å–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª–∏, —Ä–∞–∑–±–ª–æ–∫–∏—Ä—É–µ–º
      }
    } catch(e) {
      console.error(e)
      setIsAIThinking(false) // –ü—Ä–∏ –æ—à–∏–±–∫–µ —Ä–∞–∑–±–ª–æ–∫–∏—Ä—É–µ–º
      setError('–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏')
    }
  }, [isRecording, sessionId, position])

  // –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–µ—á–∏ –ò–ò
  useEffect(() => {
    // –ï—Å–ª–∏ –ò–ò –∑–∞–∫–æ–Ω—á–∏–ª –≥–æ–≤–æ—Ä–∏—Ç—å –∏ –¥—É–º–∞—Ç—å, –∏ –º–∏–∫—Ä–æ—Ñ–æ–Ω –Ω–µ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç, –≤–∫–ª—é—á–∞–µ–º –µ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    if (
      !isAISpeaking && 
      !isAIThinking && 
      !isRecording && 
      sessionId && 
      position &&
      socketService.getConnectionState() === 'connected'
    ) {
      // –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞, —á—Ç–æ–±—ã –¥–∞—Ç—å –≤—Ä–µ–º—è —Å–∏—Å—Ç–µ–º–µ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∞—É–¥–∏–æ
      const autoStartTimeout = setTimeout(() => {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—â–µ —Ä–∞–∑ –ø–µ—Ä–µ–¥ –≤–∫–ª—é—á–µ–Ω–∏–µ–º
        if (
          !isAISpeaking && 
          !isAIThinking && 
          !isRecording && 
          socketService.getConnectionState() === 'connected'
        ) {
          console.log('üé§ Auto-starting microphone after AI finished speaking')
          startRecording()
        }
      }, 500) // 500ms –∑–∞–¥–µ—Ä–∂–∫–∞

      return () => clearTimeout(autoStartTimeout)
    }
  }, [isAISpeaking, isAIThinking, isRecording, sessionId, position, startRecording])

  useEffect(() => {
    if (!sessionId || !position) return

    const initSocket = async () => {
      try {
        if (socketService.getConnectionState() === 'disconnected') {
          await interviewService.startInterview(sessionId, position)
        }

        // –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω—ã—Ö –ø–æ–ª–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        socketService.onMessage(async (data) => {
          setIsAIThinking(false)
          setAiResponse(data.text)
          setIsAISpeaking(true) // –ë–ª–æ–∫–∏—Ä—É–µ–º –º–∏–∫—Ä–æ—Ñ–æ–Ω
          await saluteFrontendService.playAudioFromText(data.text)
        })

        // --- STREAMING HANDLERS ---

        socketService.onStreamStart(() => {
          setIsAIThinking(false) // –ë–æ–ª—å—à–µ –Ω–µ –¥—É–º–∞–µ—Ç
          setIsAISpeaking(true)  // –¢–µ–ø–µ—Ä—å –≥–æ–≤–æ—Ä–∏—Ç (–±–ª–æ–∫–∏—Ä—É–µ–º –º–∏–∫—Ä–æ—Ñ–æ–Ω)
          setAiResponse("")
        })

        socketService.onStreamChunk((textChunk) => {
          setAiResponse(prev => prev + textChunk)
          saluteFrontendService.playAudioFromText(textChunk)
        })

        socketService.onStreamEnd(() => {
          // –°—Ç—Ä–∏–º –∑–∞–∫–æ–Ω—á–∏–ª—Å—è, –Ω–æ –º—ã –ù–ï —Ä–∞–∑–±–ª–æ–∫–∏—Ä—É–µ–º –º–∏–∫—Ä–æ—Ñ–æ–Ω –∑–¥–µ—Å—å.
          // –ú—ã –∂–¥–µ–º, –ø–æ–∫–∞ saluteFrontendService.onPlaybackEnded –≤—ã–∑–æ–≤–µ—Ç setIsAISpeaking(false)
          // –≠—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç, –∫–æ–≥–¥–∞ –¥–æ–∏–≥—Ä–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫—É—Å–æ—á–µ–∫ –∏–∑ –æ—á–µ—Ä–µ–¥–∏.
        })
      } catch (error) {
        setError(`–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —Å–µ—Ä–≤–µ—Ä—É: ${error}`)
      }
    }

    initSocket()

    return () => {
      interviewService.cleanup()
      saluteFrontendService.stopAudio()
      stopAudioCapture()
    }
  }, [sessionId, position])

  const stopAudioCapture = () => {
    if (scriptProcessorRef.current) {
      scriptProcessorRef.current.disconnect(); scriptProcessorRef.current = null
    }
    if (sourceRef.current) {
      sourceRef.current.disconnect(); sourceRef.current = null
    }
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop()); mediaStreamRef.current = null
    }
    if (audioContextRef.current) {
      audioContextRef.current.close(); audioContextRef.current = null
    }
  }

  const toggleRecording = useCallback(() => {
    if (!isRecording && isMicrophoneBlocked) return

    if (isRecording) {
      stopRecording()
    } else {
      startRecording()
    }
  }, [isRecording, startRecording, stopRecording, isMicrophoneBlocked])

  return {
    isRecording,
    isAIThinking,
    isAISpeaking,
    isMicrophoneBlocked,
    toggleRecording,
    transcript,
    aiResponse,
    error
  }
}

// Helpers
function downsampleBuffer(buffer: Float32Array, sampleRate: number, outSampleRate: number): Float32Array {
  if (outSampleRate === sampleRate) return buffer
  if (outSampleRate > sampleRate) return buffer
  const sampleRateRatio = sampleRate / outSampleRate
  const newLength = Math.round(buffer.length / sampleRateRatio)
  const result = new Float32Array(newLength)
  let offsetResult = 0
  let offsetBuffer = 0
  while (offsetResult < result.length) {
    const nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio)
    let accum = 0, count = 0
    for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
      accum += buffer[i]; count++
    }
    result[offsetResult] = count > 0 ? accum / count : 0
    offsetResult++; offsetBuffer = nextOffsetBuffer
  }
  return result
}

function convertFloatTo16BitPCM(buffer: Float32Array): ArrayBuffer {
  const l = buffer.length
  const buffer16 = new ArrayBuffer(l * 2)
  const view = new DataView(buffer16)
  for (let i = 0; i < l; i++) {
    let s = Math.max(-1, Math.min(1, buffer[i]))
    s = s < 0 ? s * 0x8000 : s * 0x7FFF
    view.setInt16(i * 2, s, true)
  }
  return buffer16
}
